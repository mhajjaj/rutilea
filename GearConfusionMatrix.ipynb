{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def convert_grayscale_to_3channel(input_path, output_path):\n",
    "    grayscale_image = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
    "    duplicate_image = cv2.merge((grayscale_image,) * 3)\n",
    "    cv2.imwrite(output_path, duplicate_image)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"path_to_grayscale_image.png\"\n",
    "    output_image_path = \"path_to_output_3channel_image.png\"\n",
    "\n",
    "    convert_grayscale_to_3channel(input_image_path, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def convert_grayscale_to_3channel(input_path, output_path):\n",
    "    grayscale_image = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
    "    duplicate_image = cv2.merge((grayscale_image,) * 3)\n",
    "    cv2.imwrite(output_path, duplicate_image)\n",
    "\n",
    "def batch_convert_images_in_folder(input_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all files in the input folder\n",
    "    files = os.listdir(input_folder)\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Check if the file is an image (you can add more extensions if needed)\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            convert_grayscale_to_3channel(input_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"C:\\\\Users\\\\mhajj\\\\Documents\\\\RUTILEA\\\\mhajjaj\\\\GearInspection-Dataset3\\\\CategoryNG\\\\ClassAll\\\\train\\\\images\"\n",
    "    output_folder = \"C:\\\\Users\\\\mhajj\\\\Documents\\\\RUTILEA\\\\mhajjaj\\\\GearInspection-Dataset3\\\\CategoryNG\\\\ClassAll\\\\train\\\\images\"\n",
    "\n",
    "    batch_convert_images_in_folder(input_folder, output_folder)\n",
    "    print(\"Image conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearConfusionMatrix.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W0sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W0sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W0sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W0sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torch\\optim\\optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m foreach:\n\u001b[0;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[1;32m--> 456\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_profile_name):\n\u001b[0;32m    457\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torch\\autograd\\profiler.py:507\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m    506\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 507\u001b[0m         torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_exit\u001b[39m.\u001b[39;49m_RecordFunction(record)\n\u001b[0;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torch\\_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs \u001b[39mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 460 * 700, 128)  # Adjusted input size for 920x1400\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Dummy dataset with random data (replace with your actual dataset)\n",
    "# You need to adjust the dataset loading and preprocessing according to your data\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples, num_classes, image_size=(920, 1400), transform=None):\n",
    "        self.data = torch.randn(num_samples, 3, *image_size)  # Random data, adjust to your dataset\n",
    "        self.targets = torch.randint(0, num_classes, (num_samples,))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target\n",
    "\n",
    "# Data preprocessing and data loaders (replace with your actual data)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert tensor to PIL Image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dummy_dataset = DummyDataset(num_samples=800, num_classes=8, transform=transform)\n",
    "train_size = int(0.8 * len(dummy_dataset))\n",
    "test_size = len(dummy_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dummy_dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = SimpleCNN(num_classes=8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (you should replace this with your actual training loop)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Calculate class-wise metrics using classification_report\n",
    "class_names = ['class0', 'class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7']\n",
    "class_metrics = classification_report(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Print or display the confusion matrix and class-wise metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"\\nClass-wise Metrics:\")\n",
    "for class_name, metrics in class_metrics.items():\n",
    "    if class_name in class_names:\n",
    "        print(f\"Class: {class_name}\")\n",
    "        print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"F1-Score: {metrics['f1-score']:.2f}\")\n",
    "        print(f\"Support: {metrics['support']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 256 * 320, 128)  \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x1310720 and 5242880x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearConfusionMatrix.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sample_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m8\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m640\u001b[39m)  \u001b[39m# Replace with actual input size\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Pass the sample input through the network\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m net(sample_input)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Print the shape of the output from the last convolutional/pooling layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# In this example, we assume the last layer is self.conv2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m last_layer_output_shape \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mconv2(output)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearConfusionMatrix.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Fully connected layers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x1310720 and 5242880x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Instantiate your neural network (assuming you've defined it as 'net')\n",
    "net = Net()\n",
    "\n",
    "# Create a sample input tensor with the same shape as your data\n",
    "sample_input = torch.randn(8, 1, 512, 640)  # Replace with actual input size\n",
    "\n",
    "# Pass the sample input through the network\n",
    "output = net(sample_input)\n",
    "\n",
    "# Print the shape of the output from the last convolutional/pooling layer\n",
    "# In this example, we assume the last layer is self.conv2\n",
    "last_layer_output_shape = net.conv2(output).shape\n",
    "print(\"Output shape of the last convolutional/pooling layer:\", last_layer_output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1310720, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Shape after the last pooling layer: torch.Size([8, 64, 128, 160])\n",
      "Output shape: torch.Size([8, 1])\n",
      "Number of channels in input data: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 128 * 160, 128)  # Adjust the input size based on your needs\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output size depends on your task\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "\n",
    "        # After the last pooling layer\n",
    "        print(\"Shape after the last pooling layer:\", x.shape)\n",
    "\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the custom neural network\n",
    "net = CustomNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "# Create a sample input tensor with the correct shape\n",
    "sample_input = torch.randn(8, 1, 512, 640)\n",
    "\n",
    "# Pass the sample input through the network\n",
    "output = net(sample_input)\n",
    "\n",
    "# Print the output shape to verify\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Check the number of channels\n",
    "num_channels = sample_input.shape[1]\n",
    "print(\"Number of channels in input data:\", num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearConfusionMatrix.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Initialize an empty confusion matrix\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(class_names)  \u001b[39m# Replace with the number of classes in your dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m confusion_matrix_total \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((num_classes, num_classes), dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set your model to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize an empty confusion matrix\n",
    "num_classes = len(class_names)  # Replace with the number of classes in your dataset\n",
    "confusion_matrix_total = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "# Set your model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate through the data loader\n",
    "for inputs, labels in dataloader:\n",
    "    # Make predictions using your model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    # Update the confusion matrix\n",
    "    confusion_matrix_total += confusion_matrix(labels, predicted, labels=range(num_classes))\n",
    "\n",
    "# Calculate performance metrics for each class\n",
    "class_metrics = classification_report(\n",
    "    np.arange(num_classes),\n",
    "    np.arange(num_classes),\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Print or save the confusion matrix and class-wise metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_total)\n",
    "\n",
    "print(\"Class-wise Metrics:\")\n",
    "for class_name, metrics in class_metrics.items():\n",
    "    if class_name in class_names:\n",
    "        print(f\"Class: {class_name}\")\n",
    "        print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"F1-Score: {metrics['f1-score']:.2f}\")\n",
    "        print(f\"Support: {metrics['support']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:34<00:00, 4904249.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Dummy dataset with torchvision\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for example\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = SimpleCNN(num_classes=10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  3   0   0   3   0 245   0 292 455   2]\n",
      " [  1   0   2   2   0 124   0 125 746   0]\n",
      " [  0   0   0   1   0 119   0 176 704   0]\n",
      " [  2   0   0   0   0 153   0  91 752   2]\n",
      " [  0   0   0   0   0  63   1  92 843   1]\n",
      " [  0   0   0   0   0 113   0 146 740   1]\n",
      " [  0   0   0   0   0  83   0  59 858   0]\n",
      " [  1   0   3   0   0 138   0  89 766   3]\n",
      " [  2   0   1   3   0 159   0 245 585   5]\n",
      " [  4   0   3   1   0 234   0  69 684   5]]\n",
      "\n",
      "Class-wise Metrics:\n",
      "Class: airplane\n",
      "Precision: 0.23\n",
      "Recall: 0.00\n",
      "F1-Score: 0.01\n",
      "Support: 1000.0\n",
      "Class: automobile\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n",
      "Support: 1000.0\n",
      "Class: bird\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n",
      "Support: 1000.0\n",
      "Class: cat\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n",
      "Support: 1000.0\n",
      "Class: deer\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n",
      "Support: 1000.0\n",
      "Class: dog\n",
      "Precision: 0.08\n",
      "Recall: 0.11\n",
      "F1-Score: 0.09\n",
      "Support: 1000.0\n",
      "Class: frog\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n",
      "Support: 1000.0\n",
      "Class: horse\n",
      "Precision: 0.06\n",
      "Recall: 0.09\n",
      "F1-Score: 0.07\n",
      "Support: 1000.0\n",
      "Class: ship\n",
      "Precision: 0.08\n",
      "Recall: 0.58\n",
      "F1-Score: 0.14\n",
      "Support: 1000.0\n",
      "Class: truck\n",
      "Precision: 0.26\n",
      "Recall: 0.01\n",
      "F1-Score: 0.01\n",
      "Support: 1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Training loop (assuming you've trained your model)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Calculate class-wise metrics using classification_report\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_metrics = classification_report(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Print or display the confusion matrix and class-wise metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"\\nClass-wise Metrics:\")\n",
    "for class_name, metrics in class_metrics.items():\n",
    "    if class_name in class_names:\n",
    "        print(f\"Class: {class_name}\")\n",
    "        print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"F1-Score: {metrics['f1-score']:.2f}\")\n",
    "        print(f\"Support: {metrics['support']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Simulated predictions and targets (replace with actual data)\n",
    "# predictions = torch.tensor([0, 1, 2, 0, 2, 1])\n",
    "# targets = torch.tensor([0, 1, 2, 1, 2, 0])\n",
    "\n",
    "# # Calculate the multiclass confusion matrix\n",
    "# confusion_matrix = torch.zeros(3, 3)\n",
    "\n",
    "# for t, p in zip(targets, predictions):\n",
    "#     confusion_matrix[t, p] += 1\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from super_gradients.training import Trainer, models\n",
    "from super_gradients.training.dataloaders.dataloaders import (\n",
    "    coco_detection_yolo_format_train, coco_detection_yolo_format_val\n",
    ")\n",
    "from super_gradients.training.losses import PPYoloELoss\n",
    "from super_gradients.training.metrics import DetectionMetrics_050\n",
    "from super_gradients.training.models.detection_models.pp_yolo_e import (\n",
    "    PPYoloEPostPredictionCallback\n",
    ")\n",
    "\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    #trainer params\n",
    "    HOME = os.getcwd()\n",
    "\n",
    "    CHECKPOINT_DIR = f'{HOME}\\checkpoint\\AGI-Dataset' #specify the path you want to save checkpoints to\n",
    "    EXPERIMENT_NAME = 'AGIExperiment' \n",
    "\n",
    "    ##dataset params\n",
    "    DATA_DIR = f'{HOME}\\GearInspection-Dataset3\\CategoryNG\\ClassAll' \n",
    "    LOGS = f'{CHECKPOINT_DIR}\\AGILogs'\n",
    "\n",
    "    # CATEGORY = 'CategoryNG\\ClassAll'\n",
    "\n",
    "    TRAIN_IMAGES_DIR = 'train\\images' \n",
    "    TRAIN_LABELS_DIR = 'train\\labels' \n",
    "\n",
    "    VAL_IMAGES_DIR = 'valid\\images'\n",
    "    VAL_LABELS_DIR = 'valid\\labels' \n",
    "\n",
    "    TEST_IMAGES_DIR = 'test\\images' \n",
    "    TEST_LABELS_DIR = 'test\\labels'\n",
    "\n",
    "    #what class names do you have\n",
    "    CLASSES = ['akkon', 'dakon', 'kizu', 'hakkon', 'kuromoyou', 'mizunokori', 'senkizu', 'yogore']\n",
    "\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "    DATALOADER_PARAMS={\n",
    "    'batch_size':64,\n",
    "    'num_workers':2\n",
    "    }\n",
    "\n",
    "    EPOCHS = 1\n",
    "    RUNNING_LOSS = 0.0\n",
    "    ACCURACY = 0.0\n",
    "\n",
    "    # model params\n",
    "    MODEL_NAME = 'yolo_nas_l' # choose from yolo_nas_s, yolo_nas_m, yolo_nas_l\n",
    "    PRETRAINED_WEIGHTS = 'coco' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class to load images and labels from separate folders\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        label_name = os.path.join(self.labels_dir, self.image_filenames[idx] + '.txt')  # Assuming labels are in text files\n",
    "\n",
    "        print(label_name)\n",
    "\n",
    "        image = read_image(img_name)\n",
    "        \n",
    "        # Load and process the label file (adjust as needed based on your label file format)\n",
    "        with open(label_name, 'r') as label_file:\n",
    "            label = label_file.read().strip()\n",
    "            label = int(label)  # Assuming labels are integer values\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data transformation\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create custom datasets for train, validation, and test\n",
    "train_dataset = CustomImageDataset(\n",
    "    images_dir=f'{config.DATA_DIR}\\\\{config.TRAIN_IMAGES_DIR}',\n",
    "    labels_dir=f'{config.DATA_DIR}\\\\{config.TRAIN_LABELS_DIR}',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = CustomImageDataset(\n",
    "    images_dir=f'{config.DATA_DIR}\\\\{config.VAL_IMAGES_DIR}',\n",
    "    labels_dir=f'{config.DATA_DIR}\\\\{config.VAL_LABELS_DIR}',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = CustomImageDataset(\n",
    "    images_dir=f'{config.DATA_DIR}\\\\{config.TEST_IMAGES_DIR}',\n",
    "    labels_dir=f'{config.DATA_DIR}\\\\{config.TEST_LABELS_DIR}',\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = f'{config.DATA_DIR}\\\\{config.TRAIN_IMAGES_DIR}'\n",
    "\n",
    "print(\"train_root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in c:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearInspection-Dataset3\\CategoryNG\\ClassAll\\train\\images.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearConfusionMatrix.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create custom datasets for train, validation, and test\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m ImageFolder(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     root\u001b[39m=\u001b[39;49mtrain_root,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m val_dataset \u001b[39m=\u001b[39m ImageFolder(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     root\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mVAL_IMAGES_DIR,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     transform\u001b[39m=\u001b[39mtransform\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m ImageFolder(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     root\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mTEST_IMAGES_DIR,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     transform\u001b[39m=\u001b[39mtransform\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mhajj/Documents/RUTILEA/mhajjaj/GearConfusionMatrix.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[0;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[1;32mc:\\Users\\mhajj\\Documents\\RUTILEA\\rutilea\\lib\\site-packages\\torchvision\\datasets\\folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     40\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[0;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in c:\\Users\\mhajj\\Documents\\RUTILEA\\mhajjaj\\GearInspection-Dataset3\\CategoryNG\\ClassAll\\train\\images."
     ]
    }
   ],
   "source": [
    "# Create custom datasets for train, validation, and test\n",
    "train_dataset = ImageFolder(\n",
    "    root=train_root,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "    root=config.VAL_IMAGES_DIR,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = ImageFolder(\n",
    "    root=config.TEST_IMAGES_DIR,\n",
    "    transform=transform\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rutilea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
